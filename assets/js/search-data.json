{
  
    
        "post0": {
            "title": "Pandas cheatsheet",
            "content": "Pandas is perhaps the most widely used python library for analyzing tabular data. It is based on a numpy and enables fast data analysis by using vectorized operations. Below, I highlight a number of commonly used pandas features that are useful when doing basic common data cleaning / exploration. I want to write about a few gotchas in pandas in addition to the commonly used commands. For example, integer dtype columns containing NaNs are always converted to float dtype. Another gotcha relates to replace: inplace=True does not always work with pandas replace function. . Good resources: . Amazing visual cheatsheet | Real python | Pandas docs | . Configuration . More options: docs . pd.options.plotting.backend = &#39;plotly&#39; # Maximum default rows / columns to display pd.set_option(&quot;max_rows&quot;, 50) pd.set_option(&quot;max_columns&quot;, 20) pd.set_option(&quot;max_info_columns&quot;, 50) # df.info() shows info on these many columns . Re-order columns . df = df.reindex(columns=FIRST_COLS + sorted(list(REMAINING_COLS))) . Merge / Join . pd.merge( df1, df2, how=&#39;inner&#39; ) . Assign: Create new columns easily from existing columns . df.assign( temp_f=lambda x: x[&#39;temp_c&#39;] * 9 / 5 + 32, temp_k=lambda x: (x[&#39;temp_f&#39;] + 459.67) * 5 / 9 ) . Tall to Wide form: pd.pivot . Suppose we have a dataset of student grades with each student having multiple rows for each school subject grade. We want each row to correspond to a single student entry: all grades for all subjects for the students should be in a single row of the dataframe. . df_wide = df.pivot_table(values=&#39;Grade&#39;, # Score in each subject index=[&#39;ID&#39;, &#39;School&#39;], # Student ID and school columns=&#39;Subjects&#39;, # Name of the subject aggfunc=&#39;last&#39;) . Multi-index . pivot / pivot_table return a multi-index object that has levels of indices. . df.index.get_level_values(0) . Wide to tall: pd.melt . Never used it but seems the opposite of pd.pivot_table above . https://pandas.pydata.org/docs/reference/api/pandas.melt.html . pd.melt(df, id_vars=[&#39;A&#39;], value_vars=[&#39;B&#39;, &#39;C&#39;]) . Create a binary column based on two separate columns . df.loc[df[&#39;X&#39;].notnull(), &#39;X_true&#39;] = 1 df.loc[df[&#39;X&#39;].isnull(), &#39;X_true&#39;] = 0 . One Hot Encoding: custom name for the columns . Create the columns based on OHE of a given column: . [&#39;cluster 0&#39;, &#39;cluster 1&#39;, &#39;cluster 2&#39;] . Bad way . # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]==0, &#39;cluster 0&#39;] = 1 # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]!=0, &#39;cluster 0&#39;] = 0 # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]==1, &#39;cluster 1&#39;] = 1 # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]!=1, &#39;cluster 1&#39;] = 0 # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]==2, &#39;cluster 2&#39;] = 1 # df_clusters_save.loc[df_clusters_save[&#39;clusters_kmeans&#39;]!=2, &#39;cluster 2&#39;] = 0 # df_clusters_save.columns . Good way . dummies = pd.get_dummies(df_clusters_save[&#39;clusters_kmeans&#39;]).rename(columns=lambda x: &#39;cluster &#39; + str(x)) df_clusters_save = pd.concat([df_clusters_save, dummies], axis=1) df_clusters_save.columns . Duplicates based on certain columns . duplicates = df[df.duplicated(subset=[&#39;ID&#39;, &#39;School&#39;], keep=False)] print(f&quot;Duplicates found: {duplicates.shape[0]}&quot;) # drop duplicates df_students.drop_duplicates(subset=[&#39;ID&#39;, &#39;School&#39;], keep=&#39;last&#39;, inplace=True) . Force numeric dtype . for col in df.columns.tolist()[1:]: df[col] = pd.to_numeric(df[col], errors=&#39;coerce&#39;) . Swedish/other language characters . 8: 8-bit encoding . sig: signature; apparently adds signature at the beginning of the file so that software like microsoft excel / power bi can read it correctly . df.to_csv(&#39;names_in_swedish_german_other_languages.csv&#39;, index=False, encoding=&#39;utf-8-sig&#39;) . Row entries of a column that belong to a list . For column Enhet, include/exclude only the entries that belong to a pre-defined list. . df[~df[&#39;Enhet&#39;].isin(EXCLUDE_SCHOOLS)] # Include entries df[df[&#39;Enhet&#39;].isin(INCLUDE_SCHOOLS)] . dropna based on a column . df.dropna(subset=[&#39;School&#39;], inplace=True) . Rename columns . COLUMN_NAMES = { &#39;X&#39;: &#39;Velocity&#39;, &#39;Y&#39;: &#39;Position&#39;, &#39;Z&#39;: &#39;Time&#39; } df.rename(columns=COLUMN_NAMES, inplace=True) # caveat: does NOT always work! . Replace row values . Has a lot of issues, so better to re-assign instead of using inplace=True. . SCHOOL_NAMES = { &quot;Jan&quot;: &quot;January&quot;, &quot;Jan.&quot;: &quot;January&quot; } # inplace does not always work # df.replace(SCHOOL_NAMES, inplace=True) # Better df = df.replace(SCHOOL_NAMES) . String processing . df_schools_list = df.school.str.split(&#39;-&#39;).tolist() # Get rid of white space school_names = [long_name[-1].strip() for long_name in df_schools_list] # Rename schools df.school = school_names .",
            "url": "https://fnauman.github.io/ds-blog/numpy/pandas/2021/01/07/pandas-cheatsheet.html",
            "relUrl": "/numpy/pandas/2021/01/07/pandas-cheatsheet.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Gaussian processes",
            "content": "Introduction . Gaussian processes (GPs) are an example of non-parametric Bayesian models. Non-parametric means the model has theoretically infinite parameters and does not rely on a specific functional form of the parameters. To specify a Gaussian process, one requires two things: . Mean vector. | Covariance matrix/Kernel: Relation of data points (samples) with one another. | The kernel or the covariance matrix represents the relation between the sample points. The off-diagonal elements represent dependence of one observation to another. A commonly used kernel is the Radial Basis Function, $$ k(x, x&#39;) = exp left( - frac{||x - x&#39;||^2}{2 ell^2} right) $$ where $ ell$ represents the smoothing length. . Difference between GPs and Naive Bayes: . Naive Bayes takes the &#39;weight-space&#39; approach similar to Ordinary Least Squares, $f(x) = w^T x + b$: the goal is to find the coefficients of the basis (polynomial, for example) $w^T$ such that the negative log likelihood is minimized. GPs take a &#39;function-space&#39; approach where the goal is to find the optimal $g(x) = w^T * x$: the advantage being that an explicit basis (polynomial or otherwise) is not required anymore. . Advantages: . No need for iid assumption: For most algorithms, the samples are assumed to be identically independentally distributed. Sample interdependence differs from the assumption of &#39;collinearity&#39; that refers to correlation among features. Regularization like Lasso/Ridge can get rid of collinearity among features but does not take get rid of mutually dependence in samples. Non-parametric Bayesian models address this problem by using a kernel function that quantifies interdependence of sample points. Example: time series data where adjacent samples can be strongly dependent on one another. . | Confidence intervals: GPs provide uncertainty estimates as opposed to deterministic algorithms. . | Fewer hyperparameters: The kernel has a fewer hyper parameters as opposed to a big neural network with billions of parameters to tune. . | Applications: GPs are particularly suited for: . Time series modeling (Kalman filters - do not require information about all evolution only the most recent update?) | Probabilistic numerics (Hening: Model deterministic equations numerically with uncertainty estimates) | Uncertainty quantification (Probabilistic methods, in general, are much better at this compared to deterministic algorithms). | . | Gaussian Processes are not easy to interpret since they do not use explicit features. The computational complexity of inverting the covariance matrix inversion is $n^3$ but this has been addressed by more recent approaches. . References: . Self-contained discussion with python code | scikit-learn | Application to dynamical systems | GP summer school | pyro | GPy | Example application to candy dataset . Get data . Kaggle . or . here . from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF gp = GaussianProcessRegressor(kernel=RBF()) gp.fit(X, y) y_pred, sigma = gp.predict(X, return_std=True) . Text(0, 0.5, &#39;$f(x)$&#39;) .",
            "url": "https://fnauman.github.io/ds-blog/probabilistic%20models/machine%20learning/2020/10/04/gaussian-processes.html",
            "relUrl": "/probabilistic%20models/machine%20learning/2020/10/04/gaussian-processes.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fnauman.github.io/ds-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "SINDy",
            "content": "Sparse Identification of Nonlinear Dynamical Systems (SINDy) is an algorithm to discover governing dynamical equations for time series ${ bf x}(t)$. The key idea is to construct a differential equation: . $$ frac{d{ bf x}}{d t} = Theta({ bf x}^T) { bf x}$$, . where the time derivative is computed from the time series data and $ Theta({ bf x})$ is a non-linear basis. . Steps: . Compute time derivative: Perhaps the trickiest part, especially for noisy time series although one can use total variation regularized derivatives for such cases as in here | Choose basis: Some non-linear basis constructed from the time series data, for example, polynomial basis. | Apply regularized linear regression: Apply Lasso/Ridge with one step or sequential thresholding least squares. | . Paper: https://www.pnas.org/content/113/15/3932 . Once the underlying dynamical equations are discovered, forecasting becomes a lot easier. . Extensions: . Knowing the right coordinates and basis functions to use is often difficult. The most interesting extension of SINDy so far has been to use latent basis / coordinates instead of physical space basis. In latent space, . $$ { bf x} longrightarrow { bf z} = Encoder({ bf x}).$$ . A non-linear basis in ${ bf z}$ is used to perform SINDy. The full cost function also takes into account the physical space SINDy expression. For more details, see the paper: . Paper: https://www.pnas.org/content/116/45/22445 . Example . Create synthetic dataset . Solve the ODE: . $$ begin{align} frac{dx_1}{dt} &amp;= -x_1^3 - x_2 frac{dx_2}{dt} &amp;= x_1 - x_2^3 end{align} $$ Plot the solutions . Compute the time derivative. . x1 = sol_new[:,0] x2 = sol_new[:,1] . Actual time derivatives . dx1dt = -x1**3 - x2 dx2dt = x1 - x2**3 . Numerically computed derivatives from data . Construct a basis: polynomial (or trig) . from sklearn.preprocessing import PolynomialFeatures dum_data = pd.DataFrame({&#39;x1&#39;: x1, &#39;x2&#39;: x2}) deg = 3 # Polynomial degree to use p = PolynomialFeatures(degree=deg,include_bias=True).fit(dum_data) xpoly = p.fit_transform(dum_data) newdf = pd.DataFrame(xpoly, columns = p.get_feature_names(dum_data.columns)) print(&quot;Feature names:&quot;, list(newdf))#newdf.columns.values.tolist()) print(&quot;Feature array shape:&quot;, newdf.shape) . . Feature names: [&#39;1&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;, &#39;x1^3&#39;, &#39;x1^2 x2&#39;, &#39;x1 x2^2&#39;, &#39;x2^3&#39;] Feature array shape: (1001, 10) . Regression using Lasso . (or Ridge/OLS with sequential thresholding) . Lasso does regularized linear regression with L1-norm. alpha is a hyperparameter. . low alpha -&gt; OLS | high alpha -&gt; most features zero | . from sklearn.linear_model import Lasso mod = Lasso(alpha=0.0001) mod . Lasso(alpha=0.0001) . $dx_1 / dt$ . Identified ODE: . $dx_1/dt sim -x_2 - x_1^3$ . with minor contributions from other terms that could be gotten rid of by applying a threshold . dx1_thr = fit_dx1[fit_dx1.columns[fit_dx1.abs().max() &gt; 0.1]] dx1_thr . x2 x1^3 . 0 -0.998524 | -0.502025 | . $dx_2 / dt$ . Identified ODE: . $dx_2/dt sim x_1 - x_2^3$ . with minor contributions from other terms that could be gotten rid of by applying a threshold . dx2_thr = fit_dx2[fit_dx2.columns[fit_dx2.abs().max() &gt; 0.1]] dx2_thr . x1 x2^3 . 0 0.994918 | -0.826899 | . Forecasting using SINDy . Now that we have discovered the differential equations, we can use the ODE solver to forecast the future. . def fun_forecast(y, t): x1, x2 = y dxdt = [-0.5*(x1**3) - 0.9985*x2, 0.995*x1 - 0.8269*x2**3] return dxdt t_forecast = np.linspace(10, 15, 500) y0 = [x1[-1], x2[-1]] sol_forecast = odeint(fun_forecast, y0, t_forecast) .",
            "url": "https://fnauman.github.io/ds-blog/differential%20equations/machine%20learning/2019/09/10/sindy-basic.html",
            "relUrl": "/differential%20equations/machine%20learning/2019/09/10/sindy-basic.html",
            "date": " • Sep 10, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Vectorization, broadcasting, boolean masking",
            "content": "Unlike compiled languages like C or Fortran, loops in python are quite slow. When I first started using python several years ago, if I had to compute the average over one axis (y) of a 2D function v(x,y), I would do something like this: . nx, ny = v.shape ave = np.zeros(nx) for i in range(nx): for j in range(ny): ave[i] += v[i, j] ave[i] /= ny . Then I got a little smarter and removed one loop: . for i in range(nx): ave[i] = np.mean(v[i, n:]) . But thanks to numpy ufuncs Python Data Science Handbook, I realized that I do not need any loops at all: . ave = np.mean(v, axis=1, keepdims=True) # keep the same dimensions as v . It turns out a whole lot of operations can be vectorized using numpy ufuncs. What is even better is that if you are going to repeat the same operation, you can use partial from functools to define a numpy ufunc with defaults: . from functools import partial ave_y = partial(np.mean, axis=1) ave_y(v) # averages over the 2nd axis . Broadcasting . Another problem I faced when I first started using python was dealing with arrays with mismatched dimensions. Consider a case where array x has shape (10,2) and I want to add another array y to it with shape (2). . x = np.random.randn(10, 2) # x.shape = (10,2) y = np.array([1, 2]) # y.shape = (2) # x + y won&#39;t work x + y[np.newaxis, :] # works! . Using scikit-learn: The feature array in scikit-learn API is expected to be 2 dimensional. Let&#39;s say you only want to do linear regression only on 1 feature (1D = number of samples). . from sklearn.linear_model import Lasso x = np.random.randn(10) y = 3*x + 5 + 0.1*np.random.randn(10) model = Lasso() model.fit(x, y) # Returns ValueError: Expected 2D array, got 1D array... # Reshape your data either using array.reshape(-1, 1) if # your data has a single feature or array.reshape(1, -1) . Scikit-learn throws a helpful error message with a suggested fix: model.fit(x.reshape(-1,1), y). You can also use x[:, np.newaxis]. . Alternatively, one might want to use two feature arrays of dimension 1 in scikit-learn. Scikit-learn expects dimension 2 feature arrays. . x1 = np.random.randn(10) x2 = np.arange(10) # Efficient way: column_stack X = np.column_stack((x1, x2)) # Long, inefficient way X = np.zeros((x1.shape, 2)) X[:, 0] = x1 X[:, 1] = x2 . For more on broadcasting, see Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow and Python Data Science Handbook . Boolean masking and array dimensions . x = np.linspace(1, 10, 10) x &gt; 5 # array([False, False, False, False, False, True, True, True, True, True]) x[x &gt; 5] # array([ 6., 7., 8., 9., 10.]) . A conditional statement x&gt;5 generates a boolean array of the same dimension as the original array. When we use x[x&gt;5], only the values satisfying the conditional statement are returned. In many applications, it is not a problem but if you have code that breaks if the array dimensions don&#39;t match anymore, you want to simply multiply the boolean array: . x * (x &gt; 5) # array([ 0., 0., 0., 0., 0., 6., 7., 8., 9., 10.]) . xarray: A lot of these vectorization and broadcasting operations are made easy by xarray where as a bonus, parallelism is included using dask. A good tutorial on the utility of xarray is here. .",
            "url": "https://fnauman.github.io/ds-blog/numpy/pytorch/2019/09/09/numpy-vectorization-and-broadcasting.html",
            "relUrl": "/numpy/pytorch/2019/09/09/numpy-vectorization-and-broadcasting.html",
            "date": " • Sep 9, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Farrukh Nauman, a data scientist in Gothenburg, Sweden. I am interested in machine learning and machine learning - physics interface. My background is in computational astrophysics. See my CV here. .",
          "url": "https://fnauman.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fnauman.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}